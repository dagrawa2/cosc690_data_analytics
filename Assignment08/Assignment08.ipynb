{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 594 / CS 690 - Assignment 08\n",
    "### October 22, 2018\n",
    "---\n",
    "\n",
    "For this assignment, you must work in groups of one or two students. Each person is responsible to write their own code, but the group will (together) discuss their solution.  In this notebook, we provide you with basic functions for completing the assignment.  *Complete the assignment in this notebook.  You will need to modify existing code and write new code to find a solution*.  Each member of the group must upload their own work (i.e., a notebook file) to GitHub.\n",
    "\n",
    "*Note: Running a cell will not rerun previous cells.  If you edit code in previous cells, you must rerun those cells.  If you are having trouble with undefined errors and code changes not applying, we recommend using* `Run All` *to avoid any errors results from not rerunning previous cells.  You can find this in the menu above:* `Cell -> Run All`\n",
    "\n",
    "Previously, we looked at food items reported in the dietary data from the National Health and Nutrition Examination Survey (NHANES).  As a reminder, NHANES is a cross-sectional survey that is conducted every two years in the United States. As part of the survey, individuals are asked to complete a demographics questionnaire and a 24-hour dietary recall. The 24-hour dietary recalls have been shown to be a valid and reliable method for describing usual dietary intakes of a population.  Given the tens of thousands of different foods an individual may report in the NHANES dietary recall data, it is beneficial to group (i.e., cluster) similar foods based on macronutrient and micronutrient content.  These food groups can be used to gain new insight into the dietary patterns of individuals and populations.  For example, if an individual eats only foods in the \"high fat, low carbohydrate\" group we may conclude that this individual is on the [ketogenic diet](https://en.wikipedia.org/wiki/Ketogenic_diet).\n",
    "\n",
    "Last assignment, we provided data with food items and fat and carbohdryate contents of each food item.  You may have visually noticed that many of the food clusters (found with $k$-Means) were very close and not very distinct (i.e., well separated).  In this assignment, we provide raw NHANES dietary intake sample data from 2 years of the survey.  These data include many more values for each food item, including micronutrient content (e.g., minerals and vitamins) and metadata (e.g., person reporting the food and time of reporting).  The inclusion of all nutrient content values increases the dimensionality of the data (i.e., the number of values representing each food item is large) and should also produce more distinct clusters.  Given the high-dimensionality of the data, it seems appropriate to use a clustering algorithm more complex than the $k$-Means we have used in previous assignments.\n",
    "\n",
    "In this assignment you will use [Density-based spatial clustering of applications with noise](https://en.wikipedia.org/wiki/DBSCAN) (DBSCAN) to cluster food items based on macronutrient and micronutrient content.  You learned about DBSCAN in today's lecture, but if you want to learn more, here is an [interactive DBSCAN model](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/).  We will reference the paper *[Development of a Scalable Method for Creating Food Groups Using the NHANES Dataset and MapReduce](https://dl.acm.org/citation.cfm?id=2975179)* (the ACM Digital Library is available through the University, but may not be available at home.  We have included the paper in this repo at `./paper.pdf`), which also used DBSCAN to cluster food items from the NHANES dataset.  You will use the data preprocessing code and the DBSCAN code provided in this paper's [git repo](https://github.com/TauferLab/NHANES-Analytics).\n",
    "\n",
    "*You will need to preprocess the raw NHANES dietary data (located at `./data/NHANES-20**-dietary.csv`), cluster it with DBSCAN, and reproduce figure 6 from the \"Development of a Scalable Method for Creating Food Groups Using the NHANES Dataset and MapReduce\" paper.  Note: your figures may be slightly different because you are only using 2 years of NHANES data, where the paper used 7 years of data.*\n",
    "\n",
    "We have isolated the relevant code from the paper, adapted it for this assignment, and included it in this repository.  If you would like to look at the source code, open the python files located in `./src/`, or visit the [git repo](https://github.com/TauferLab/NHANES-Analytics).  Below, we load a SparkContext and provide an example of how to load the NHANES dietary data, preprocess it, and cluster it.  Note that we also load a text file, `./data/features.txt` which contains the list of micro- and macronutrirent values we want from the raw NHANES data.  Use this code as a reference for how to use the provided preprocessing and clustering code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of food entries: 68500\n",
      "List of features:\n",
      " ['DR1IFDCD', 'SEQN', 'DR1MC', 'DR1IGRMS', 'DR1IKCAL', 'DR1IPROT', 'DR1ICARB', 'DR1ISUGR', 'DR1IFIBE', 'DR1ITFAT', 'DR1ISFAT', 'DR1IMFAT', 'DR1IPFAT', 'DR1ICHOL', 'DR1IATOC', 'DR1IATOA', 'DR1IRET', 'DR1IVARA', 'DR1IACAR', 'DR1IBCAR', 'DR1ICRYP', 'DR1ILYCO', 'DR1ILZ', 'DR1IVB1', 'DR1IVB2', 'DR1INIAC', 'DR1IVB6', 'DR1IFOLA', 'DR1IFA', 'DR1IFF', 'DR1IFDFE', 'DR1ICHL', 'DR1IVB12', 'DR1IB12A', 'DR1IVC', 'DR1IVD', 'DR1IVK', 'DR1ICALC', 'DR1IPHOS', 'DR1IMAGN', 'DR1IIRON', 'DR1IZINC', 'DR1ICOPP', 'DR1ISODI', 'DR1IPOTA', 'DR1ISELE', 'DR1ICAFF', 'DR1ITHEO', 'DR1IALCO', 'DR1IMOIS', 'DR1IS040', 'DR1IS060', 'DR1IS080', 'DR1IS100', 'DR1IS120', 'DR1IS140', 'DR1IS160', 'DR1IS180', 'DR1IM161', 'DR1IM181', 'DR1IM201', 'DR1IM221', 'DR1IP182', 'DR1IP183', 'DR1IP184', 'DR1IP204', 'DR1IP205', 'DR1IP225', 'DR1IP226']\n"
     ]
    }
   ],
   "source": [
    "# Note: If you are having trouble loading Spark, try uncommenting the following two lines\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Load code from paper\n",
    "from src.preprocess import cleanNHANESData\n",
    "from src.DBSCAN import DBSCAN\n",
    "\n",
    "# Load NHANES dietary data from 2009 and 2011\n",
    "# The preprocessing code expects a dictionary with RDDs of NHANES dietary data\n",
    "raw_data = {}\n",
    "for year in ['2009', '2011']:\n",
    "    raw_data[year] = sc.textFile('./data/NHANES-{}-dietary.csv'.format(year))\n",
    "    \n",
    "# Print total number of food item entries\n",
    "total = 0\n",
    "for year_data in raw_data.values():\n",
    "    total += year_data.count()\n",
    "print('Total number of food entries: {}'.format(total))\n",
    "    \n",
    "# Load list of nutrient features we want from the raw dietary data\n",
    "with open('./data/features.txt') as f:\n",
    "    features = f.readlines()\n",
    "features = [f.strip() for f in features]\n",
    "\n",
    "# Look at the list of features you will use for each food item\n",
    "# We need this list because some features (i.e., values) in the raw NHANES data cannot be used to cluster\n",
    "# Note that the first 4 features are:\n",
    "# \"USDA food code\", \"Respondent sequence number\", \"Modification code\", and \"Grams\"\n",
    "# The remaining features are micro- and macronutrient features\n",
    "# HINT: ref [1]\n",
    "print('List of features:\\n', features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that the first four features are not related to nutrient values.  These features are used in the cleaning process and will be removed post-processing. \n",
    "\n",
    "Apply the pre-processing to the data!  You will notice that there are tens of thousands food item entries.  We are only interested in the unique food items (e.g., if a person reports *bread* twice, we only care about one instance of *bread*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique food items: 3968\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 1 times, most recent failure: Lost task 0.0 in stage 23.0 (TID 90, localhost, executor driver): java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat java.io.DataOutputStream.write(Unknown Source)\r\n\tat java.io.FilterOutputStream.write(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:201)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:213)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:213)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:213)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:407)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:215)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1991)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:170)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:149)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat java.io.DataOutputStream.write(Unknown Source)\r\n\tat java.io.FilterOutputStream.write(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:201)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:213)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:213)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:213)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:407)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:215)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1991)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:170)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b64757622906>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Elements in RDD are key-value pairs of (food ID, np.array(nutrient value))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Total unique food items: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1374\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1375\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1377\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   1011\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1013\u001b[1;33m         \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1014\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 1 times, most recent failure: Lost task 0.0 in stage 23.0 (TID 90, localhost, executor driver): java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat java.io.DataOutputStream.write(Unknown Source)\r\n\tat java.io.FilterOutputStream.write(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:201)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:213)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:213)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:213)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:407)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:215)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1991)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:170)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:149)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketException: Connection reset by peer: socket write error\r\n\tat java.net.SocketOutputStream.socketWrite0(Native Method)\r\n\tat java.net.SocketOutputStream.socketWrite(Unknown Source)\r\n\tat java.net.SocketOutputStream.write(Unknown Source)\r\n\tat java.io.BufferedOutputStream.write(Unknown Source)\r\n\tat java.io.DataOutputStream.write(Unknown Source)\r\n\tat java.io.FilterOutputStream.write(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:201)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:213)\r\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:213)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:213)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:407)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$anonfun$run$1.apply(PythonRunner.scala:215)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1991)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:170)\r\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing code from paper\n",
    "# This may take a minute!\n",
    "clean_data = cleanNHANESData(sc, raw_data, features).persist()\n",
    "\n",
    "# Look at cleaned NHANES dietary data\n",
    "# Elements in RDD are key-value pairs of (food ID, np.array(nutrient value))\n",
    "print ('Total unique food items: {}'.format(clean_data.count()))\n",
    "print(clean_data.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data returned from `cleanNHANESData()` is an RDD of `(key, values)` where the keys are food item IDs (these uniquely identify each unique food item) and the values are micro- and macronutrient values!  Some of the nutrient values are negative.  This is because the nutrient values are [normalized and standardized](https://en.wikipedia.org/wiki/Normalization_%28statistics%29).  Now, apply the DBSCAN clustering below!  **This may take several minutes, depending on your machine!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-aee3ca14931d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Cluster the food items with DBSCAN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# This may take a few minutes, depending on your machine!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mfood_clusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDBSCAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclean_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminpts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_pts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Look at the cluster results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\COSC690\\dagrawa2\\Assignment08\\src\\DBSCAN.py\u001b[0m in \u001b[0;36mDBSCAN\u001b[1;34m(sc, data, epsilon, minpts, metric)\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_FindMinNeighbor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0m_TestConverge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_neighbor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m             \u001b[0mconverge\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0mmin_neighbor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\COSC690\\dagrawa2\\Assignment08\\src\\DBSCAN.py\u001b[0m in \u001b[0;36m_TestConverge\u001b[1;34m(rdd1, rdd2)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_TestConverge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrdd2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mrdd1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[0mrdd1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_TestSame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mrdd1\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    860\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m         \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    832\u001b[0m         \"\"\"\n\u001b[0;32m    833\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 834\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    835\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    836\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1255\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m             \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Answer received: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define DBSCAN parameters\n",
    "epsilon = 1\n",
    "min_pts = 4\n",
    "metric = 'euclidean'\n",
    "\n",
    "# Cluster the food items with DBSCAN\n",
    "# This may take a few minutes, depending on your machine!\n",
    "food_clusters = DBSCAN(sc, clean_data, epsilon=epsilon, minpts=min_pts, metric=metric)\n",
    "\n",
    "# Look at the cluster results\n",
    "# Elements in the RDD are key-value pairs of (food ID, cluster ID)\n",
    "print('Clustered food items: {}'.format(food_clusters.count()))\n",
    "print(food_clusters.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting RDD from DBSCAN contain `(key, value)` pairs.  The `key` is the ID of the food item in the cluster.  The `value` is the cluster ID.  This version of DBSCAN labels clusters by the food item with the *minimum* ID value in a cluster.  Last, you will see that the number of food items clustered is smaller than the total number of unique food items because DBSCAN removes *noise* (i.e., food items that are not similar to other food items)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1:\n",
    "\n",
    "Recreating Figure 6 from the paper should be accomplished in incremental steps.  Now that you know how to preprocess and cluster food items based on nutrient content using DBSCAN, you can develop metrics to report in the figure.  Figure 6 reports (1) the percentage of food items clustered and (2) the number of food clusters found.  *Define the functions below which intake food clusters (i.e., the output of DBSCAN) and returns percentage of food items clustered and number of food clusters found.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to report the percent of total food items that are clustered\n",
    "# The input 'clusters' should be the output of DBSCAN\n",
    "# The input 'total' should be the total number of unique food items (before clustering)\n",
    "# HINT: ref [2]\n",
    "def percentClustered(clusters, total):\n",
    "    return 100*clusters.count()/total\n",
    "    \n",
    "# Define a function to count the number of clusters\n",
    "# The input 'clusters' should be the output of DBSCAN\n",
    "# HINT: ref [2,3]\n",
    "def clusterCount(clusters):\n",
    "    return clusters.map(lambda x: x[1]).countDistinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the functions you defined about to report the percent of food items clustered and the number of food clusters from the clusters you found above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply the functions you defined to the clusters found above\n",
    "perc_clust = percentClustered(food_clusters, clean_data.count())\n",
    "clust_cnt = clusterCount(food_clusters)\n",
    "\n",
    "# Print results\n",
    "print('Food items clustered: {:.2f}%'.format(perc_clust))\n",
    "print('Number of food clusters: {}'.format(clust_cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample Output:**\n",
    "\n",
    "```\n",
    "Food items clustered: 34.68%\n",
    "Number of food clusters: 79\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "- [1: NHANES 2011 Nutrient Labels](https://wwwn.cdc.gov/Nchs/Nhanes/2011-2012/DR1IFF_G.htm)\n",
    "- [2: RDD.count](https://spark.apache.org/docs/2.1.0/api/python/pyspark.html?highlight=count#pyspark.RDD.count)\n",
    "- [3: RDD.distinct](https://spark.apache.org/docs/2.1.0/api/python/pyspark.html?highlight=count#pyspark.RDD.distinct)\n",
    "\n",
    "## Don't forget to shelve your Jetstream instance when you're not using it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2:\n",
    "\n",
    "Now that you have a method to recreate the values found in Figures 6 of the paper, you must create a plotting script.  *Below, define a function that intakes a 2D array and plots a heatmap.*  We provide `dummy_data` for you to test your plotting method.  Remember, you should label the axes, provide the correct axes tick labels, and give the heatmap a title!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAADxCAYAAADr5V2tAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAHZFJREFUeJzt3XmUZGWd5vHvYwGyiLIUsm8CgyBH\nAUvUQRFBsKQR1HaU6lbRRtAeaMDREdEz6tjjDEwrigeVLgHBDaVR2mpltZVBbFGyAFnEBVnLQooS\nKUBQKOuZP+4NCJLIzBtRcW/mzXg+58TJuDfeuL83suCXb7z3XWSbiIhoj6dNdwUiIqI/SdwRES2T\nxB0R0TJJ3BERLZPEHRHRMkncEREtk8QdEdEySdwRES2TxB0R0TJrTHcFIiKmw/z58718+fJKZRcv\nXnyJ7fk1V6myJO6IGEnLly9nbGysUllJc2uuTl+SuCNiRBlYOd2VGEgSd0SMKAN/mu5KDCSJOyJG\nVFrcEREtk8QdEdEySdwRES2TxB0R0UJJ3BERLbIK+PN0V2IgSdwRMaLSVRIR0UJJ3BERLZIWd0RE\nyyRxR0S0zCqGNeVd0lnAwcAy27uV574B7FwW2QC43/buPd57O/Ag8Bdgpe15U8VL4o6IETa0FvfZ\nwGnAlzonbL+581zSJ4EVk7z/lbarrTFLEndEjKzhdZXYvkLSdr1ekyTgTcB+QwlGdsCJiJHVSdxV\nHqvl5cA9tn89SUUulbRY0lFVLpgWd0SMqL5a3HMlde+6sND2worvXQCcO8nre9teKunZwGWSfmH7\niskumMQdESOqr8S9vMpNw/EkrQG8AXjhhLWwl5Y/l0m6ANgLmDRxp6skIkZUZyOFKo+BvQr4he0l\nvV6UtJ6k9TvPgQOBG6e6aBJ3RIyo4fVxSzoX+DGws6Qlko4oXzqMcd0kkraQdGF5uClwpaSfAT8F\nvmv74qnipaskIkaUKYZOD+FK9oIJzr+9x7mlwEHl81uBF/QbL4k7IkZUZk5GRLRQEndERIsMb8p7\n03JzMiKiZZK4ZxhJt0t6RNKDku6X9B+S3i2pdf9Wkj4q6bHyszwo6VeSTpO0eR/XuFzSO1ejDmdL\n+l/jzm0nyeUY29WyuvWL6dTYzMmha10yGBGvtb0+sC1wEnACcOb0Vmlg3yg/y0bA64HNgMX9JO+I\neiRxRw1sr7C9CHgzcLikznKRT2rlSXq7pCu7ji3pv0r6ddnS/UdJO0j6saQHJJ0naa2y7L7luNP3\nS1om6W5Jr5N0UNlCvk/SB8uym0l6WNLGXbFeKOleSWtO8Vkes31T+VnuBd5bvn9DSd8pr/GH8vlW\n5Wsfp1jn4TRJD0k6rTx/qqS7ys+yWNLLV+f3LOnpkj4h6U5J90g6XdI6q1G/fn7/E16/fP1ySf9H\n0k8lrZD0bUkbrc7njW5J3FET2z8FllAkiarmU0yzfQnwfmAh8LfA1sBuFOsndGwGrA1sCXwY+ALw\nlvL9Lwc+LOk5tn8HXE6x0lnHW4Cv236s4mf5C/Dtrs/yNOCLFN8utgEeoVgeE9sfAn4IHGP7GbaP\nKd9zNbA7RSv+a8C/SFq7SvwJnAz8p/KaO/LE72HQ+kH13/+E1+/yNuDvgC0osshnVuOzxuPS4o76\nLaVIVFWdbPuBspV7I3Cp7VttrwAuAvboKvsY8PEy+X4dmAucavvB8v03Ac8vy55DkayRNIciAX15\n0M9i+/e2v2n7YdsPAh8HXjHZm21/pXzfStufBJ7OEwvW9/K+8n7B/ZLuB67vvCBJwJHAe2zfV9bh\nf1PMeBuofqVKv/+K1/+y7Rtt/xH4H8Cbyt99rJbOqJJap7zXIom7PbYE7uuj/D1dzx/pcfyMruPf\nly3hzmu93t8p/21gV0nPAQ4AVpTfCPrx+GeRtK6kf5Z0h6QHKBbX2WCyxCTpvZJuLrsO7geeRfHH\nZiKfsL1B58ETf4QANgHWpeh37yT2i8vzA9WvVOn3X/H6d3U9vwNYc4rPG5WkxR01kvQiimTX6cf+\nI0Wy6disqbrY/hNwHsXX/rfSZ2tbxeiY11J0MUDR170z8GLbzwT26RTthBz3/pdT3Kx9E7BhmYhX\ndJXv13KKRPq8ruT+LNudP1R91W8AU10fiu6Vjm0oviFV3i0lJpPEHUMm6ZmSDqbovviK7RvKl64D\n3lC21nYEjpjwIvX4EvB24BDgK1XeIGlNSbtQLLizGXBK+dL6FInz/vKm20fGvfUe4Dldx+tT/J90\nL7CGpA8DzxzsY4DtVRR9+p9SsR4ykraU9OoB69evqa4P8BZJu0paF/gYcH7XN6QYWFrcMVz/JulB\niq/IH6JIcu/oev1TwKMUSeMc4KtNVs72jyg6CK+xffsUxd8s6SHgfmAR8HvghZ01iIFPA+tQtCCv\nouim6HYq8MZyxMVngEso+oh/RdFt8Cee3JUwiBOAW4Cryu6K7/FEn3m/9evXVNeH4lvN2cDvKG4i\nHztAnHiK9iZu2av7TS9GkaTvA1+zfcZ012U2k3Q5xbet/J6HbN68uR4bO6RSWemLiwfZSKEuWask\n+lb2ue8JHDrddYkYnIE/T3clBpLEHX2RdA7wOuC4cvhaREtlWdcYEbYPn+46jBLb+053HWavJO6I\niJZJ4o6IaJkk7qGYO1febtuGgmmbhgLBtYvvbCTOHs9qJAwA165oLtYeL1ynsVgPLn5k6kJD0sx/\nFcWMnaasv2czcW6/A5Yv96CTrkqdXd5Xn6SzgIOBZbY7i8F9lGI5hXvLYh+0fWGP986nGFY6BzjD\n9klTxZtRiXu7bWHsJw0FW+ODDQWC9fTuRuKMVVlBY0jWW9RcrLGxnRqL9X1dP3WhITmuoTinNhQH\nYL+G/v+d9+JhXGWoLe6zKRYH+9K485+y/YmJ3lQubfBZiuUjlgBXS1pk++eTBcsEnIgYUcObgGP7\nCvpbS6hjL+CWcgGyRylmSU85zDaJOyJGVCMzJ4+RdL2ksyRt2OP1LXnyzN8l5blJJXFHxAirnLjn\nShrrehxV4eKfB3agWOf9buCTPcr06qefcjr7jOrjjohoTl993Mv7nfJu+/GlfCV9AfhOj2JLePLq\nj1tRrFc/qSTuiBhRnY0U6iFpc9t3l4evp9hQY7yrgZ0kbQ/8lmIDj7+Z6tpJ3BExooY3qkTSucC+\nFF0qSyiW591X0u5loNuBd5Vlt6AY9neQ7ZWSjqFY9XIOcFa5a9KkkrgjYoQNZ1lz2wt6nD5zgrJL\ngYO6ji8EnjK+ezJJ3BExoto7c7LWUSWS5kv6paRbJH2gzlgREf1p70YKtbW4B50RFBHRjPa2uOvs\nKnl8RhCApM6MoCTuiJgBhrdWSdPqTNy9ZgQNZYWBiIjVlxZ3L5VmBJUzkI4C2KbJZcwiYsS1N3HX\neXOy0owg2wttz7M9b5O5NdYmImI8/6XaY4apM3E/PiNI0loUM4IaXAw0ImIKqyo+ZpjaukoGnREU\nEdGIVcCj012JwdQ6AWeQGUEREY2Zga3pKjJzMiJGkxnWjPfGJXFHxOhKizsiokXS4o6IaKEk7oiI\nFjHw2HRXYjBJ3BExmtJVEhHRQrk5GRHRImlxD8eSa+CENZuJdbLf1Uwg4BTe3Uic7ze4oMCrmwtF\nk4tK7sj1jcW64ZBm4vxzg/9d7PfXDQX6zZCukxZ3RESLpMUdEdEyQxxVIuks4GBgme3dynP/BLyW\nYkWU3wDvsH1/j/feDjxI8Wdkpe15U8Wrdc/JiIgZq9PirvKY2tnA/HHnLgN2s/184FfAiZO8/5W2\nd6+StCGJOyJG2ZASt+0rgPvGnbvUdmenhqso9iQYiiTuiBhNpsn1uP8OuGiSmlwqaXG5I9iU0scd\nEaOr+s3JuZLGuo4X2l5Y5Y2SPkSxR9pXJyiyt+2lkp4NXCbpF2ULfkJJ3BExmvq7Obm8av9zN0mH\nU9y03N/2U/bcBbC9tPy5TNIFwF7ApIk7XSURMZqGe3PyKSTNB04ADrH98ARl1pO0fuc5cCBw41TX\nTuKOiFhNks4FfgzsLGmJpCOA04D1Kbo/rpN0ell2C0mdncE2Ba6U9DPgp8B3bV88Vbzaukp6jWuM\niJhRhjRz0vaCHqfPnKDsUuCg8vmtwAv6jVdni/tsnjquMSJiZqi5q6ROde7yfoWk7eq6fkTEapuB\nSbmKaR9VUo5bPArgmdNcl4gYIdlIYXDlWMiFAJtJPYfLREQMXWcCTgtNe+KOiJg26SqJiGiRFi/r\nWtuokgnGNUZEzBzNrVUyVHWOKuk1rjEiYmZYRbFSdgulqyQiRtcMbE1XkcQdEaOpxX3cSdwRMbrS\n4o6IaJG0uCMiWiiJOyKiRTLlPSKiZVo85X3KCTiSdpD09PL5vpKOlbRB/VWLiKjZLF7W9ZvAPEk7\nUiwMvgj4GuVC4MO01bPg5FcM+6oTqbSZ8lDcOsviAHxrlwaDvecLjYXaxkc2FusNauZzfctXNxKn\nUfPetvrXaPHNySpT3lfZXgm8Hvi07fcAm9dbrYiIBsziKe+PSVoAHA68tjy3Zn1ViohowCxvcb8D\neCnwcdu3Sdoe+Eq91YqIqFlnVEmVxwwzZeK2/XPbx9o+tzy+zfZJ9VctIqJGQ9xzUtJZkpZJurHr\n3EaSLpP06/LnhhO89/CyzK8lHV6l6hMmbknnlT9vkHT9+EeVi0dEzGjD6+M+m6dujv4B4N9t7wT8\ne3n8JJI2Aj4CvBjYC/jIRAm+22R93MeVPw+eus4RES0zxD7uCTZHPxTYt3x+DnA5cMK4Mq8GLrN9\nH4Ckyyj+AJw7WbwJE7ftu8un69n+efdrkvYF7pjswhERM171xD1X0ljX8cJyv9zJbNrJo7bvlvTs\nHmW2BO7qOl5SnptUlVEl50n6MvB/gbXLn/MoblhGRLRTf1Pel9ueV0Mt1OPclJumVxlV8mJga+A/\ngKuBpcDeU9ZG2lrSDyTdLOkmScdN9Z6IiMYM8ebkBO6RtDlA+XNZjzJLKPJrx1YUOXZSVRL3Y8Aj\nwDoULe7bbFfprl8JvNf2LsBLgKMl7VrhfRERzah3As4iivkvlD+/3aPMJcCBkjYsb0oeWJ6bVJXE\nfTVF4n4R8DJggaTzp3qT7bttX1M+fxC4mQp9NxERjRjucMBem6OfBBwg6dfAAeUxkuZJOgOgvCn5\njxR59mrgY50blZOp0sd9hO1Op/zvgEMlvbXC+7o/1HbAHsBP+nlfRESthjSdfZLN0ffvUXYMeGfX\n8VnAWf3EqzIB5/E7qZLWk/S3wGFVA0h6BsVCVcfbfqDH60dJGpM0dm9Ld1yOiBaqv4+7NlWWdV1L\n0uvKCTl3A68CTq9ycUlrUiTtr9r+Vq8ythfanmd73iZr9VHziIjV0eIp7xN2lUg6AFhAMUD8B8CX\ngb1sv6PKhSWJYhnYm22fMoS6RkQM1wxsTVcxWYv7EmAH4GW232L73+ivR2hv4K3AfpKuKx9DX8M7\nImIgLe4qmezm5Asp+rK/J+lW4OvAnKoXtn0lvQeXR0TMDDNwre0qJmxx277W9gm2dwA+SjEqZC1J\nF0lqbvuYiIg6tLjFXWUcN7Z/ZPsYinHYnybT3SOi7TqbBc/SHXAeV86YvIQKM3siImY0Ay0dgtxX\n4o6ImFVmYGu6isk2Uriwx/qyERGzwyzt4z4buFTSh8qJNBERs0tLE/dkGymcJ+m7wIeBsXJN7lVd\nr2dSTUS0V+fmZAtN1cf9GPBH4OnA+rT2Y0ZEjNPfRgozymRT3ucDp1CsKbun7YfrrsyjK+DORXVH\nKWzzni80Ewg4+TMNBfpeQ3GAExr6dwI4+edHNheswQUsv3V8M3FO0IuaCQSc3FSn6sohXWcGdoNU\nMVmL+0PAf7F9U1OViYhozBA3C27aZH3cL2+yIhERjWtp52/GcUfEaJqNLe6IiFktiTsiomVaPKqk\n0iJTERGz0pAWmZK0c9e+A9dJekDS8ePK7CtpRVeZDw9a7bS4I2I0DbGrxPYvgd0BJM0Bfgtc0KPo\nD20fvLrxkrgjYnTV08e9P/Ab23fUcnXSVRIRo6q+9bgPA86d4LWXSvpZuSHN8waoNVBji1vS2sAV\nFNPl1wDOt/2RuuJFRPSteot7rqSxruOFtheOLyRpLeAQ4MQe17gG2Nb2Q+X+u/8K7NRfhQt1dpX8\nGdivrOSawJWSLrJ9VY0xIyKq6W9UyXLb8yqUew1wje17nhLOfqDr+YWSPidpru3llWtRqi1x2zbw\nUHm4ZvlwXfEiIvpSzzjuBUzQTSJpM+Ae25a0F0VX9e8HCVLrzcny7upiYEfgs7abW8EnImIqQ5zy\nLmld4ADgXV3n3g1g+3TgjcDfS1oJPAIcVjZw+1Zr4rb9F2B3SRsAF0jazfaN3WXKHeOPgmIn4oiI\nRgy5xV2uoLrxuHOndz0/DThtGLEaGVVi+37gcmB+j9cW2p5ne95GTVQmIqKjpTvg1Ja4JW1StrSR\ntA7wKuAXdcWLiOhL5+ZklccMU2dXyebAOWU/99OA82x/p8Z4ERHVZZGpp7J9PbBHXdePiFhtWY87\nIqJdWtrgTuKOiNHU4p6SJO6IGF0t7SlJ4o6I0bQK+NN0V2JASdwRMZLSVRIR0ULpKomIaJG0uCMi\nWqitiVsDLk5VizmS124o1h+fWPulAZ9rMFZTzmwu1Fovai7WJ5sLNSsNtC1A/+YdC2O/slbnGs+X\nvKhi2e1hccX1uBuRFndEjKTOzmVtlMQdESOrrV0lSdwRMZJyczIiooXSVRIR0SJpcUdEtEx/m7zP\nLEncETGS0uKOiGihYfZxS7odeJDi78HK8eO+JQk4FTgIeBh4u+1rBomVxB0RI6mmFvcrbS+f4LXX\nUExR2gl4MfD58mffat/lXdIcSddKyn6TETGjNLzJ+6HAl1y4CthA0uaDXKj2xA0cB9zcQJyIiMo6\nMyerPPq45KWSFks6qsfrWwJ3dR0vKc/1rdbELWkr4K+AM+qMExHRLwOPVnwAcyWNdT16Jea9be9J\n0SVytKR9xr3ea22VgRaLqruP+9PA+4H1a44TEdG3PlrTy6daZMr20vLnMkkXAHsBV3QVWQJs3XW8\nFbC0ehWeUFuLW9LBwDLbi6cod1Tnr9jMWacwIma7zs3JYfRxS1pP0vqd58CBwI3jii0C3qbCS4AV\ntu8epO51trj3Bg6RdBCwNvBMSV+x/ZbuQrYXAguhWNa1xvpERDzJEIcDbgpcUIz4Yw3ga7YvlvRu\nANunAxdSDAW8hWI44DsGDVZb4rZ9InAigKR9gfeNT9oREdNlmMMBbd8KvKDH+dO7nhs4ehjxMo47\nIkZSprxPwfblwOVNxIqIqCpT3iMiWiRrlUREtFDW446IaJG0uCMiWiabBUdEtExnynsbJXFHxMhK\nizsiokXSxx0R0UJJ3EOwCpY/DHf0+ba5wEQ7TkyoXEKgXwPFGlBTsQaM86IGYw1ksFjHNhhr5sZp\nQ6xtVzdobk4Oie1N+n2PpLGpllscltkYazZ+ptkaazZ+pqZjjZcWd0REi2StkoiIlsnNyem1MLFa\nESex2hNnNsd6krb2catYIjYiYrRsIfnIimU/Bounqx++l9nQ4o6I6Fubu0pq3eW9TpLmS/qlpFsk\nfaDmWGdJWiZp/B5yw46ztaQfSLpZ0k2Sjqsx1tqSfirpZ2Ws/1lXrDLeHEnXSvpOnXHKWLdLukHS\ndZLGaoyzgaTzJf2i/Dd7aU1xdi4/S+fxgKTj64hVxntP+d/EjZLOlbR2TXGOK2PcVOfnmUjn5mSV\nx0zTysQtaQ7wWeA1wK7AAkm71hjybGB+jdfvWAm81/YuwEuAo2v8XH8G9rP9AmB3YH65gWldjgNu\nrvH6473S9u41f709FbjY9nMptq2q5fPZ/mX5WXYHXkixX+EFdcSStCXFaPZ5tncD5gCH1RBnN+BI\nip3QXwAcLGmnYceZyqqKj5mmlYmb4h/7Ftu32n4U+DpwaF3BbF8B3FfX9bvi3G37mvL5gxSJYMua\nYtn2Q+XhmuWjlhsekrYC/go4o47rTwdJzwT2Ac4EsP2o7fsbCL0/8Bvb/U5U68cawDqS1gDWBZbW\nEGMX4CrbD9teCfw/4PU1xJnQMHd5b1pbE/eWwF1dx0uoKcFNF0nbAXsAP6kxxhxJ1wHLgMts1xXr\n08D7aa7xYuBSSYslHVVTjOcA9wJfLLuAzpC0Xk2xuh0GnFvXxW3/FvgEcCdwN7DC9qU1hLoR2EfS\nxpLWpdj9fOsa4kxqWIm7SjenpH0lrejq8vrwoPVua+JWj3OzZniMpGcA3wSOt/1AXXFs/6X8+r0V\nsFf59XWoJB0MLLO9eNjXnsTetvek6Eo7WtI+NcRYA9gT+LztPYA/AnXfa1kLOAT4lxpjbEjx7XV7\nYAtgPUlvGXYc2zcDJwOXARcDP6PoKmxMZ8r7kLpKqnZz/rDT7WX7Y4PWva2JewlP/uu8FfV8nWuc\npDUpkvZXbX+riZjlV/zLqacff2/gEEm3U3Rp7SfpKzXEeZztpeXPZRR9wXvVEGYJsKTrW8r5FIm8\nTq8BrrF9T40xXgXcZvte248B3wL+cx2BbJ9pe0/b+1B0Rf66jjiTGVaLu8luTmhv4r4a2EnS9mUr\n5DBg0TTXabVJEkWf6c22T6k51iaSNiifr0PxP+wvhh3H9om2t7K9HcW/0/dtD70F1yFpPUnrd54D\nB1J8LR8q278D7pK0c3lqf+Dnw44zzgJq7CYp3Qm8RNK65X+P+1PTTVdJzy5/bgO8gfo/25P0Oapk\nrqSxrseEXXBTdHO+tBzJdZGk5w1a91aO47a9UtIxwCUUd73Psn1TXfEknQvsS/GPtwT4iO0zawi1\nN/BW4Iay7xngg7YvrCHW5sA55QidpwHn2a59qF4DNgUuKHIOawBfs31xTbH+Afhq2Xi4FXhHTXEo\n+4EPAN5VVwwA2z+RdD5wDcXX/2upb2bjNyVtTJEbj7b9h5ri9NTnOO7lVUYoTdHNeQ2wre2HJB0E\n/Csw0EiazJyMiJE0V/LBFcueU2HmZNnN+R3gkirfmMvuw3m2+14+t5Ut7oiI1TXMmZNVujklbQbc\nY9uS9qL4pvv7QeIlcUfEyBriGO2e3ZzANgC2TwfeCPy9pJXAI8BhHrDLI10lETGSNpK8f8Wy52eR\nqYiI6beKmbkOSRVJ3BExsmbidPYq2jqOO1qknA58m6SNyuMNy+OeG75Ker0kS3puhWvPk/SZYdc5\nZr8hz5xsVBJ31M72XcDngZPKUycBCydZKGkBcCUVVqWzPWZ7sL3ZY+RlkamIyX2KYkbe8cDLgE/2\nKlROYNgbOIKuxF22wr+nwuaSfiVps3Lhnu+UZV7RtYDPtZ0ZlBG9tHl1wPRxRyNsPybpv1MsKHRg\nuRxvL6+jWOP6V5Luk7Sn7WtsXyDpr4GjKdZU+Yjt343rTnkfxQy8H5V/AP5U52eKdmvzLu9pcUeT\nXkOxVOhkqxAuoFiMivLngq7X/gE4Efiz7V7rWvwIOEXSscAG5TrPERNKiztiEpJ2p1hr4yXAlZK+\nbvvucWU2BvYDdpNkinVoLOn95USFLSnuFW0q6Wm2n3TfyPZJkr5LsbbzVZJeZXvoC2fF7NC5OdlG\naXFH7crpwJ+nWHjnTuCfKBbrH++NwJdsb2t7O9tbA7cBLyt3Y/ki8DcUq9X9tx5xdrB9g+2TgTFg\nylEpMdra2uJO4o4mHAncafuy8vhzwHMlvWJcuQU8dS/Fb1Ik6w9SLEL/Q4qk/U5Ju4wre7yKzWd/\nRjGl+KJhfoiYXdo8HDBT3iNiJK0nuepXsmsy5T0iYvq1eVRJEndEjKRhLuvatCTuiBhJSdwRES00\nE288VpHEHREjKS3uiIgWSos7IqJFDEy0YM5Ml8QdESMpU94jIlpomFPeJc2X9EtJt0j6QI/Xny7p\nG+XrP5G03aD1TuKOiJE0zPW4Jc0BPkuxAuauwAJJu44rdgTwB9s7UqxPf/KgdU/ijoiRNcS1SvYC\nbrF9a7nW/NeBQ8eVORQ4p3x+PrB/uQBb35K4I2Ikdaa8V3lUsCVwV9fxkvJczzLlWvErgI0HqXtu\nTkbESFoFl/wR5lYsvraksa7jhbYXdh33ajmPX8GvSplKkrgjYiTZnj/Eyy0Btu463gpYOkGZJeX6\n8s8C7hskWLpKIiJW39XATpK2l7QWxUbXi8aVWQQcXj5/I/B9D7iudlrcERGryfZKSccAl1BsuXeW\n7ZskfQwYs70IOBP4sqRbKFrahw0aLxspRES0TLpKIiJaJok7IqJlkrgjIlomiTsiomWSuCMiWiaJ\nOyKiZZK4IyJaJok7IqJl/j/ON8zZ+iSCjwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x59810f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# define a class to help normalize heatmap scale\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "class MidpointNormalize(Normalize):\n",
    "\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
    "        return np.ma.masked_array(np.interp(value, x, y))\n",
    "\n",
    "# Define a function that intake a 2D array of data values and plots a heatmap\n",
    "# Make sure you include axes and tick labels as well as a color bar!\n",
    "# HINT: ref [1,2,3]\n",
    "def plotHeatMap(data, x_title='X Axis', y_title='Y Axis', title='', x_ticks=[], y_ticks=[]):\n",
    "    vmin, midpoint = np.min(data), np.mean(data)\n",
    "    plt.imshow(data, interpolation='nearest', cmap=plt.cm.hot, norm=MidpointNormalize(vmin=vmin, midpoint=midpoint))\n",
    "    plt.xlabel(x_title)\n",
    "    plt.ylabel(y_title)\n",
    "    plt.colorbar()\n",
    "    plt.xticks(np.arange(len(x_ticks)), x_ticks)\n",
    "    plt.yticks(np.arange(len(y_ticks)), y_ticks)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "# Test plotting function with dummy data\n",
    "dummy_data = np.random.randint(20, size=(5,10))\n",
    "plotHeatMap(dummy_data, x_ticks=range(10), y_ticks=range(5), title='Dummy Data Heatmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, your heatmap should look similar to this:\n",
    "<img src=\"./sample_heatmap.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "- [1: pyplot.pcolor](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.pcolor.html)\n",
    "- [2: Making a heatmap with pcolor example](https://stackoverflow.com/questions/14391959/heatmap-in-matplotlib-with-pcolor)\n",
    "- [3: pyplot.colorbar](https://matplotlib.org/api/colorbar_api.html)\n",
    "\n",
    "## Don't forget to shelve your Jetstream instance when you're not using it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3:\n",
    "\n",
    "Now that you have methods for calculating the values in Figure 6 from the paper and a method to plot the values, you can recreate the figure!  **To save time, you will recreate a figure similar to Figure 6 in the paper, but with only 30% of the the data.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample 30% of the original data\n",
    "clean_data_sample = clean_data.sample(False, 0.30).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In the cell below, cluster food items with the euclidean distance metric and `epsilon` and `min_pts` values in ranges $[2,4]$ and $[4,7]$, respectively.  Plot two heatmaps.  One heatmap should show the percentage of food items clustered and the second heatmap should show the number of clusters found.*  Note: We use values for `epsilon` and `min_pts` that are different than those in the paper because we are using a smaller, less dense dataset.  The figures you make will be different than Figure 6 from the paper.  In the cell below, we provide two arrays.  Fill these arrays with the appropriate values and use them to create your heatmaps.  **This may take several minutes, depending on your machine!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the distance metric we want to use\n",
    "metric = 'euclidean'\n",
    "\n",
    "# Test each combination of epsilon and minpts value in the ranges defined above!\n",
    "# Fill each of the numpy arrays below with the correct values for epsilon i and min_pts j\n",
    "# For example, when epsilon is 4 and min_pts is 5, enter values in array location [2,1]\n",
    "hm_per_data = np.zeros((3, 4))\n",
    "hm_count_data = np.zeros((3, 4))\n",
    "\n",
    "# Created nested for loops to enumerate each combination of epsilon and min_pts value\n",
    "# Record the percentage of food items clustered and the number of clusters\n",
    "# We reccommend you use print statements to see the progress!\n",
    "for i, epsilon in enumerate(range(2, 5)):\n",
    "    for j, min_pts in enumerate(range(4, 8)):\n",
    "        print(\"Clustering with epsilon = \", epsilon, \" and min_pts = \", min_pts, \" . . .\")\n",
    "      food_clusters = DBSCAN(sc, clean_data, epsilon=epsilon, minpts=min_pts, metric=metric)\n",
    "        perc_clust = percentClustered(food_clusters, clean_data.count())\n",
    "        clust_cnt = clusterCount(food_clusters)\n",
    "        print('Food items clustered: {:.2f}%'.format(perc_clust))\n",
    "        print('Number of food clusters: {}'.format(clust_cnt))\n",
    "        hm_per_data[i, j] = perc_clust\n",
    "        hm_count_data[i, j] = clust_cnt\n",
    "        \n",
    "# Plot the results in heatmaps\n",
    "plotHeatMap(hm_per_data, y_title='epsilon', x_title='min_pts',\\\n",
    "            title='Percent Food Items Clustered', y_ticks=range(2,5), x_ticks=range(4,8))\n",
    "plotHeatMap(hm_count_data, y_title='epsilon', x_title='min_pts',\\\n",
    "            title='Numer of Food Clusters', y_ticks=range(2,5), x_ticks=range(4,8))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Don't forget to shelve your Jetstream instance when you're not using it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4:\n",
    "\n",
    "While we cannot visualize the 40+ dimensions of nutrient data present, we can project the data (and clusters) down to 2 dimensions and visualize the clusters.  Select an `epsilon` and `min_pts` values that you think clusters the food items best (based on your heatmaps in Problem 3).  *Define values for `epsilon` and `min_pts` in the cell below and examine the resulting clusters.  Try different values for the DBSCAN input parameters and see how the clusters change!*  Below we provide code to cluster the data with DBSCAN and project the nutrient values to 2 Dimensions for visual assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Projects and plots high dimensional food clusters to a 2D axis\n",
    "# Input: clusters: RDD <<(cluster ID, [nutrient, values]), (cluster ID, [nutrient, values]), ...>>\n",
    "# Return: None (shows a plot)\n",
    "def plotFoodClusters2D(clusters):\n",
    "    # Get cluster IDS to color plotted clusters\n",
    "    cluster_ids = clusters.keys().collect()\n",
    "    unique_ids = np.unique(cluster_ids).tolist()\n",
    "    cluster_ids = [unique_ids.index(i) for i in cluster_ids]\n",
    "    \n",
    "    # Reduce nutrient values to 2 dimensions with TSNE\n",
    "    # HINT: ref [1,2]\n",
    "    nutrient_values = clusters.values().collect()\n",
    "    cluster_embedded = TSNE(n_components=2).fit_transform(nutrient_values)\n",
    "    \n",
    "    # Plot clusters\n",
    "    X = cluster_embedded[:,0]\n",
    "    Y = cluster_embedded[:,1]\n",
    "    plt.scatter(X, Y, c=cluster_ids, s=3)\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    \n",
    "    \n",
    "# Define your values for epsilon and min_pts\n",
    "#epsilon = \n",
    "#min_pts = \n",
    "metric = 'euclidean'\n",
    "\n",
    "# Cluster the food items with your parameters and DBSCAN\n",
    "food_clusters = DBSCAN(sc, clean_data_sample, epsilon=epsilon, minpts=min_pts, metric=metric)\n",
    "\n",
    "# Combine cluster ID with nutrient value\n",
    "# RDD is now << (food ID, (cluster ID, [nutrient, values])), (food ID, (cluster ID, [nutrient, values])), ...>>\n",
    "cluster_data = food_clusters.join(clean_data_sample)\n",
    "\n",
    "# Isolate cluster ID and nutrient value (i.e., get rid of food item ID)\n",
    "# RDD should be <<(cluster ID, [nutrient, values]), (cluster ID, [nutrient, values]), ...>>\n",
    "cluster_nutrient = cluster_data.values()\n",
    "\n",
    "# Plot clusters in 2D projected space and report cluster information\n",
    "plotFoodClusters2D(cluster_nutrient)\n",
    "perc_clust = percentClustered(food_clusters, clean_data_sample.count())\n",
    "clust_cnt = clusterCount(food_clusters)\n",
    "print('{} Clusters found with {:.2f} % food items clustered'.format(clust_cnt, perc_clust))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "- [1: TSNE Algorithm](https://lvdmaaten.github.io/tsne/)\n",
    "- [2: sklearn's TSNE](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)\n",
    "\n",
    "## Don't forget to shelve your Jetstream instance when you're not using it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5:\n",
    "\n",
    "For this assignment and previous assignments we have been using [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance), but this distance metric can suffer from the [Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality).  Other distance metrics may be better suited for this high-dimensional NHANES food data, such as [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity).  Below, we provide code that clusters the food items with DBSCAN using the *Cosine Similarity* metric.  Examine the clusters found using DBSCAN and *Cosine Similarity*, then answer the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define values for epsilon and min_pts\n",
    "epsilon = 0.1\n",
    "min_pts = 4\n",
    "metric = 'cosine'\n",
    "\n",
    "# Cluster the food items with your parameters and DBSCAN\n",
    "food_clusters = DBSCAN(sc, clean_data_sample, epsilon=epsilon, minpts=min_pts, metric=metric)\n",
    "\n",
    "# Combine cluster ID with nutrient value\n",
    "cluster_data = food_clusters.join(clean_data_sample)\n",
    "\n",
    "# Isolate cluster ID and nutrient value (i.e., get rid of food item ID)\n",
    "cluster_nutrient = cluster_data.values()\n",
    "\n",
    "# Plot clusters in 2D projected space and report cluster information\n",
    "plotFoodClusters2D(cluster_nutrient)\n",
    "perc_clust = percentClustered(food_clusters, clean_data_sample.count())\n",
    "clust_cnt = clusterCount(food_clusters)\n",
    "print('{} Clusters found with {:.2f} % food items clustered'.format(clust_cnt, perc_clust))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the clusters you found during last assignment with $k$-means and only two macronutrients to the cluster you found with DBSCAN and all the macronutrients and micronutrients.  What differences do you notice?  Which do you think is better, and why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answers here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In Problem 4, you chose an `epsilon` and `min_pts` value based on the heatmaps you created in Problem 3.  What was your motivation for choosing these values?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answers here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this assignment, you clustered food items using DBSCAN with the Euclidean distance metric and Cosine Similarity.  What differences did you notice?  Which do you believe is better, and why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answers here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DBSCAN can only find clusters of a certain density, based on the `epsilon` and `min_pts` value provided.  However, some datasets may contain clusters with different densities.  How could the DBSCAN algorithm be improved for data containing clusters of different densities?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answers here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things to Consider:\n",
    "Many machine learning tasks have labeled data which indicates if a model correctly predicts an outcome.  With clustering, there are typically no labels for the datasets.  How can we be sure that our clusters are good?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Questions:\n",
    "**Answer the following questions, in a couple sentences each, in the cells provided below**\n",
    "* List the key tasks you accomplished during this assignment?\n",
    "* Describe the challenges you faced in addressing these tasks and how you overcame these challenges?\n",
    "* Did you work with other students on this assignment? If yes, how did you help them? How did they help you? Be as specific as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answers here*\n",
    "\n",
    "## Don't forget to shelve your Jetstream instance when you're not using it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project\n",
    "\n",
    "**It is time to answer these key questions for your project:** \n",
    "* What is your dataset?\n",
    "\n",
    "I will be using the \"Sentiment140\" dataset that comprises 1.6 million tweets (<https://www.kaggle.com/kazanova/sentiment140>).\n",
    "\n",
    "* What are the scientific question(s) that you want to answer in your project? \n",
    "\n",
    "What are the key topics that people have been talking about on Twitter, and how have people's attitudes changed about these topics over time? Is it possible to predict people's sentiments on these topics going forward?\n",
    "\n",
    "* What is the (tentative) methodology? Are you clustering, or classifying? How can you look for solutions in an efficient and distrbuted way?\n",
    "\n",
    "I will use LDA to find the most important topics. Using the sentiment labels of the tweets, I will plot the proportion of positive tweets over time for each topic. Based on the observed trends, I will choose a regression model to predict sentiment over time for each topic.\n",
    "\n",
    "* What are the metrics of success?\n",
    "\n",
    "I will be looking for trends in sentiment over time. I will fit a regression model for each topic and apply it on a test set. If I get good prediction on the test set, then I can conclude that it is possible to predict the attidues of people as a function of time on each topic.\n",
    "\n",
    "* What are the milestones you want to meet from now until Dec 3 when you will present your poster at the poster showcase?\n",
    "\n",
    "\t1. The dataset is too big for this project. I will plot a histogram of tweet time stamps as well as sentiments to pick a reasonable subset of the data to work with-- one that is small enough to work with but also has patterns that can be useful.\n",
    "\t2. I will preprocess the text-- throw out stop words, handle punctuation, etc. I will then format the tweets as a TFIDF matrix.\n",
    "\t3. I will perform LDA.\n",
    "\t4. I will assign a sentiment score to each topic for each month in the data.\n",
    "\t5. I will perform regression on sentiments over time for each topic and look for trends.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
