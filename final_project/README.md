---
title: Sentiment Analysis of Important Topics on Twitter
author: Devanshu Agrawal
---

This README is a guide to reproduce the results presented in "../final_poster". It provides descriptions of the scripts in this directory that need to be run to reproduce the results. Any .py scripts that are not discussed in this README are not important.

# run.sh

This is a shell script that when run produces results that should closely match those presented in "../final_poster". The script creates the necessary subdirectories, downloads the data, preprocesses the data, runs the LDA model, generates results, and draws plots.

The most important difference between the results generated by run.sh and the results presented in "../final_poster" is the topic names. By default, the topics are named "Topic 1", "Topic 2", . . . . In order to give the topics more meaningful names such as those in "../final_poster", after running model.py, open "results/topic_summaries.txt" and interpret the listed words. Then open "results/topic_names.json" and rename the topics based on interpretations of the words listed in "results/topic_summaries.txt". Then rerun all scripts in run.sh listed after model.py.

After everything is run, the key results are found in the following files:
	- plots/
	- results/topic_summaries_with_names.txt
	- results/example_tweets.txt

Subsequent sections describe the scripts that are run by run.sh.

# utils.py

This script just contains some very simple helper functions for loading and saving JSON files.

# init.sh

This script creates all necessary subdirectories and downloads the data. The data is a CSV file located in the data subdirectory.

# pd.py

Here "pd" stands for "process data". This script loads the data and takes a random sample of the tweets. It parses the date into month and year and also assigns an integer time stamp where the earliest date is 0 and subsequent integers represent subsequent days.

The script also calculates an average sentiment score for each day.

The results of this script are saved in the processed_data subdirectory.

# pc.py

Here "pc" stands for "process corpus". This script preprocesses the tweets sampled by pd.py by tokenizing and removing stop words.

The results of this script are saved in the processed_corpus subdirectory.

# model_ho.py

This script performs LDA on the preprocessed corpus of tweets for various hyperparameter combinations and records the log-likelihood of each run in a CSV file. Topic summaries (i.e., a list of salient topic and the key words defining topics) are also generated for each run. The log-likelihood CSV and all the topic summaries are saved in the results subdirectory.

# heat.py

This script produces a heatmap of the log-likelihoods collected by model_ho.py. The plot is saved in plots subdirectory.

# model.py

This script performs LDA on the preprocessed corpus of tweets and returns the document-topic matrix, topic-word matrix, and topic summaries. The script also generates a list of topic names. By default, the names are "Topic 1", "Topic 2", . . . . All results are saved in the results subdirectory. After this script is run, the user should rename the topics in results/topic_names.json based on the words listed in "results/topic_summaries.txt".

# tsne.py

This script performs TSNE on a fraction of tweets based on their topic distributions and generates a plot. The plot is saved in the plots subdirectory.

# sent.py

This script calculates a sentiment score for each topic. The resulting array of scores is saved in the results subdirectory.

# sent_plot.py

This script plots the sentiment breakdown by topic obtained from sent.py as a bar graph. The plot is saved in the plot subdirectory.

# sent_evol.py

This script calculates a topic-wise sentiment breakdown similar to sent.py, but it is done separately for each day. The resulting array is saved in the results subdirectory.

# sent_evol_plot.py

This script plots the evolution of sentiment for each topic based on the results of sent_evol.py. The plot is saved in the plots subdirectory.

# nts.py

Here "nts" stands for "named topic summaries". This script takes the summaries in results/topic_summaries.txt and produces a new summaries file where the topics are given meaningful names based on results/topic_names.json. The new list of summaries is saved in the results subdirectory.

# et.py

Here "et" stands for "example tweets". This script takes a random sample of tweets and prints them to a file along with the predicted topic label and the probability of that topic. The result is saved in the results subdirectory.
